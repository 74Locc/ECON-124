rm(list=ls())
set.seed(5678)
r<-100
n<-1000
beta_hat_avar <- numeric(r)
beta_hat_avar_robust <- numeric(r)
beta_0 <- numeric(r)
beta_1 <- numeric(r)
beta_hat_1 <- numeric(r)
beta_hat_0 <- numeric(r)
for (j in 1:r){
x <- rnorm(n,0,1)
u <- rnorm(n,0,1)
gamma <- 1
vv <- (1 + gamma*x)*u
y <- beta_0 + beta_1*x + vv
beta_hat_1[j] <- cov(x,y)/var(x)
beta_hat_0[j] <- mean(y) - beta_hat_1[j]*mean(x)
res <- y-beta_hat_0[j] - beta_hat_1[j]*x
sigma_hat_2 <- mean(res^2)
beta_hat_avar[j] <- sigma_hat_2/var(x)/n
beta_hat_avar_robust[j] <- sum((x - mean(x))^2*res^2)/(sum((x-mean(x))^2))^2
}
mean(beta_hat_1)
mean(beta_hat_avar)
mean(beta_hat_avar_robust)
#Part B
CIlow_robust <- beta_hat_1 - 1.96*sqrt(beta_hat_avar_robust)
CIupper_robust <- beta_hat_1 + 1.96*sqrt(beta_hat_avar_robust)
reject_robust <- as.logical((CIlow_robust > 1) + (CIupper_robust < 1))
CIlow <- beta_hat_1 - 1.96*sqrt(beta_hat_avar)
CIupper <- beta_hat_1 + 1.96*sqrt(beta_hat_avar)
reject <- as.logical((CIlow > 1) + (CIupper < 1))
CIlow_robust <- beta_hat_1 - 1.96*sqrt(beta_hat_avar_robust)
CIupper_robust <- beta_hat_1 + 1.96*sqrt(beta_hat_avar_robust)
reject_robust <- as.logical((CIlow_robust > 1) + (CIupper_robust < 1))
CIlow <- beta_hat_1 - 1.96*sqrt(beta_hat_avar)
CIupper <- beta_hat_1 + 1.96*sqrt(beta_hat_avar)
reject <- as.logical((CIlow > 1) + (CIupper < 1))
rm(list = ls())
n <- 500  #size of sample
pi_0 <- 1 #setting parameter values (for simplicity, all = 1)
pi_1 <- 1
beta_0 <- 1
beta_1 <- 1
#rho <- 0 # this makes OLS consistent
rho <- 0.9 #this is a parameter for dependency between u and v in (3)
##      ^^ means v and u have strong correlation
S <- 1000 # number of iterations in the loop
set.seed(7777)
OLS <- numeric(S)
IV  <- numeric(S)
for(j in 1:S){
z <- rnorm(n,0,1)  # generating standard normal distributions
u <- rnorm(n,0,1)
h <- rnorm(n,0,1)
v <- rho*u + (1-rho)*h  # v is generated  as a weighted average of u and h
x <- pi_0 + pi_1*z + v  # store B_1 parameter into the container
y <- beta_0 + beta_1*x + u
ols_estimate <- lm(y~x)  # storing ols estimator of y on x
OLS[j] <- coefficients(ols_estimate)["x"]
iv_estimate <- ivreg(y~x | z)  #this is the instrumental variable regression, and put z after | since its the IV
IV[j] <- coefficients(iv_estimate)["x"] # storing B_1 from IV regression into IV container
}
##c.)
#rho <- 0 # this makes OLS consistent
plot(density(OLS), xlim = c(0.7, 1.7))
lines(density(IV), col = 'red')
rm(list=ls())
set.seed(5678)
r<-100
n<-1000
beta_hat_avar <- numeric(r)
beta_hat_avar_robust <- numeric(r)
beta_0 <- numeric(r)
beta_1 <- numeric(r)
beta_hat_1 <- numeric(r)
beta_hat_0 <- numeric(r)
for (j in 1:r){
x <- rnorm(n,0,1)
u <- rnorm(n,0,1)
gamma <- 1
vv <- (1 + gamma*x)*u
y <- beta_0 + beta_1*x + vv
beta_hat_1[j] <- cov(x,y)/var(x)
beta_hat_0[j] <- mean(y) - beta_hat_1[j]*mean(x)
res <- y-beta_hat_0[j] - beta_hat_1[j]*x
sigma_hat_2 <- mean(res^2)
beta_hat_avar[j] <- sigma_hat_2/var(x)/n
beta_hat_avar_robust[j] <- sum((x - mean(x))^2*res^2)/(sum((x-mean(x))^2))^2
}
#Part a.)
mean(beta_hat_1)
mean(beta_hat_avar)
mean(beta_hat_avar_robust)
#Part b.)
CIlow_robust <- beta_hat_1 - 1.96*sqrt(beta_hat_avar_robust)
CIupper_robust <- beta_hat_1 + 1.96*sqrt(beta_hat_avar_robust)
reject_robust <- as.logical((CIlow_robust > 1) + (CIupper_robust < 1))
CIlow <- beta_hat_1 - 1.96*sqrt(beta_hat_avar)
CIupper <- beta_hat_1 + 1.96*sqrt(beta_hat_avar)
reject <- as.logical((CIlow > 1) + (CIupper < 1))
rm(list = ls()) #cleaing the enviroment
install.packages("AER")  # Install stargrazer package
library(AER)  # Load AER packages to run IV commands
#####   Question 1   #####
## Consider the following model:
# y = beta_0 + beta_1*x + u   ----(1)
# x = pi_0 + pi_1*z + v       ----(2)
# v = rho*u + (1-rho)*h       ----(3)
# ^^ (3) describes relationship between error terms of (1) and (2)
### a.)
# B^_1,OLS is consistent only when Cov(x,u) = 0, if Cov(x,u) != 0 then it would be a bias estimate of B_1 and not consistant
# B^_1,IV is consistent if Cov(x,u) != 0 since z will not be correlated with error term
### b.)
n <- 500  #size of sample
pi_0 <- 1 #setting parameter values (for simplicity, all = 1)
pi_1 <- 1
beta_0 <- 1
beta_1 <- 1
#rho <- 0 # this makes OLS consistent
rho <- 0.9 #this is a parameter for dependency between u and v in (3)
##      ^^ means v and u have strong correlation
S <- 1000 # number of iterations in the loop
set.seed(7777)
OLS <- numeric(S)
IV  <- numeric(S)
for(j in 1:S){
z <- rnorm(n,0,1)  # generating standard normal distributions
u <- rnorm(n,0,1)
h <- rnorm(n,0,1)
v <- rho*u + (1-rho)*h  # v is generated  as a weighted average of u and h
x <- pi_0 + pi_1*z + v  # store B_1 parameter into the container
y <- beta_0 + beta_1*x + u
ols_estimate <- lm(y~x)  # storing ols estimator of y on x
OLS[j] <- coefficients(ols_estimate)["x"]
iv_estimate <- ivreg(y~x | z)  #this is the instrumental variable regression, and put z after | since its the IV
IV[j] <- coefficients(iv_estimate)["x"] # storing B_1 from IV regression into IV container
}
##c.)
#rho <- 0 # this makes OLS consistent
plot(density(OLS), xlim = c(0.7, 1.7))
lines(density(IV), col = 'red')
rm(list = ls())
library(AER)
n <- 500  #size of sample
pi_0 <- 1 #setting parameter values (for simplicity, all = 1)
pi_1 <- 1
beta_0 <- 1
beta_1 <- 1
#rho <- 0 # this makes OLS consistent
rho <- 0.9 #this is a parameter for dependency between u and v in (3)
##      ^^ means v and u have strong correlation
S <- 1000 # number of iterations in the loop
set.seed(7777)
OLS <- numeric(S)
IV  <- numeric(S)
for(j in 1:S){
z <- rnorm(n,0,1)  # generating standard normal distributions
u <- rnorm(n,0,1)
h <- rnorm(n,0,1)
v <- rho*u + (1-rho)*h  # v is generated  as a weighted average of u and h
x <- pi_0 + pi_1*z + v  # store B_1 parameter into the container
y <- beta_0 + beta_1*x + u
ols_estimate <- lm(y~x)  # storing ols estimator of y on x
OLS[j] <- coefficients(ols_estimate)["x"]
iv_estimate <- ivreg(y~x | z)  #this is the instrumental variable regression, and put z after | since its the IV
IV[j] <- coefficients(iv_estimate)["x"] # storing B_1 from IV regression into IV container
}
##c.)
#rho <- 0 # this makes OLS consistent
plot(density(OLS), xlim = c(0.7, 1.7))
lines(density(IV), col = 'red')
n <- 500  #size of sample
pi_0 <- 1 #setting parameter values (for simplicity, all = 1)
pi_1 <- 1
beta_0 <- 1
beta_1 <- 1
#rho <- 0 # this makes OLS consistent
rho <- 0.9 #this is a parameter for dependency between u and v in (3)
##      ^^ means v and u have strong correlation
S <- 1000 # number of iterations in the loop
set.seed(7777)
OLS <- numeric(S)
IV  <- numeric(S)
for(j in 1:S){
z <- rnorm(n,0,1)  # generating standard normal distributions
u <- rnorm(n,0,1)
h <- rnorm(n,0,1)
v <- rho*u + (1-rho)*h  # v is generated  as a weighted average of u and h
x <- pi_0 + pi_1*z + v  # store B_1 parameter into the container
y <- beta_0 + beta_1*x + u
ols_estimate <- lm(y~x)  # storing ols estimator of y on x
OLS[j] <- coefficients(ols_estimate)["x"]
iv_estimate <- ivreg(y~x | z)  #this is the instrumental variable regression, and put z after | since its the IV
IV[j] <- coefficients(iv_estimate)["x"] # storing B_1 from IV regression into IV container
}
##c.)
#rho <- 0 # this makes OLS consistent
plot(density(OLS), xlim = c(0.7, 1.7))
lines(density(IV), col = 'red')
n <- 500  #size of sample
pi_0 <- 1 #setting parameter values (for simplicity, all = 1)
pi_1 <- 1
beta_0 <- 1
beta_1 <- 1
#rho <- 0 # this makes OLS consistent
rho <- 0.9 #this is a parameter for dependency between u and v in (3)
##      ^^ means v and u have strong correlation
S <- 1000 # number of iterations in the loop
set.seed(7777)
OLS <- numeric(S)
IV  <- numeric(S)
for(j in 1:S){
z <- rnorm(n,0,1)  # generating standard normal distributions
u <- rnorm(n,0,1)
h <- rnorm(n,0,1)
v <- rho*u + (1-rho)*h  # v is generated  as a weighted average of u and h
x <- pi_0 + pi_1*z + v  # store B_1 parameter into the container
y <- beta_0 + beta_1*x + u
ols_estimate <- lm(y~x)  # storing ols estimator of y on x
OLS[j] <- coefficients(ols_estimate)["x"]
iv_estimate <- ivreg(y~x | z)  #this is the instrumental variable regression, and put z after | since its the IV
IV[j] <- coefficients(iv_estimate)["x"] # storing B_1 from IV regression into IV container
}
##c.)
#rho <- 0 # this makes OLS consistent
plot(density(OLS), xlim = c(0.7, 1.7))
lines(density(IV), col = 'red')
rm(list = ls()) #clearing the enviroment
library(dplyr)  # Data Manipulation
install.packages("wooldridge")
library(wooldridge)
data("ceosal1", package = 'wooldridge')
###################################################
library(dplyr)  # Data Manipulation
## Eliseo Vega
## ECON-124
## Professor Leung
## January 8, 2022
#### Homework 1 ####
getwd()
setwd("/Users/elis_imac/Desktop/ECON-124_Winter_2022/Problem Sets/HomeWork1")
install.packages("readstata13")
library(readstata13)
######     Question 1    ######
## a.)
install.packages("readstata13")
library(readstata13)
fakenews <- read.dta13("fakenews.dta")
survey <- read.dta13("survey.dta")
######     Question 2    ######
help("na.omit")
# x = number of times pro-Trump articles were shared on Facebook
fakenews <- na.omit(fakenews)
x <- sum(fakenews$fb_share[fakenews$pro == "Trump"])
x
# y = number of times pro-Clinton articles were shared on Facebook
y <- sum(fakenews$fb_share[fakenews$pro == "Clinton"])
y
# Using cat() function to make a dynamic print message
# Multiplying x and y by 1000 since number of articles was in 1000s
cat(x*1000,"pro-Trump articles were shared on Facebook, and ", y*1000, "pro-Clinton articles were shared, in this dataset.")
######   Question 3   ######
# creating value 'Totalbuzz' for number of articles in fakenews
Totalbuzz <- sum(fakenews$buzzfeed == 1)
# creating value 'Justbuzz' for buzz articles that are not in any other data base
Justbuzz <- Totalbuzz - sum(fakenews$buzzfeed == 1 & (fakenews$politifact == 1 | fakenews$snopes == 1))
# percent of buzz articles that dont appear in others
Justbuzz / Totalbuzz
Totalsnopes <- sum(fakenews$snopes == 1)
Justsnopes <- Totalsnopes - sum(fakenews$snopes == 1 & (fakenews$politifact == 1 | fakenews$buzzfeed == 1))
Justsnopes / Totalsnopes
Totalpol <- sum(fakenews$politifact == 1)
Justpol <- Totalpol - sum(fakenews$politifact == 1 & (fakenews$snopes ==1 | fakenews$buzzfeed ==1))
Justpol / Totalpol
######   Question 4   ######
# Package to make numbers percentages
install.packages("formattable")
library(formattable)
TotalMinutes <- sum(survey$MediaMinutesPerDay)
TotalMinutes
SocialMinutes <- sum(survey$SocialMediaMinutesPerDay)
SocialMinutes
#using percent function with 0 allowable decimals
SocialPercent <- percent(SocialMinutes/TotalMinutes,0)
cat("Respondents reported spending a total of ", TotalMinutes, " minutes consuming
election news, and ",SocialPercent, "of that time was spent on Social Media" )
install.packages("readstata13")
## Eliseo Vega
## ECON-124
## Professor Leung
## January 8, 2022
#### Homework 1 ####
getwd()
##setwd("/Users/elis_imac/Desktop/ECON-124_Winter_2022/Problem Sets/HomeWork1")
setwd("/Users/eliseovega/Desktop/ECON-124_Winter_2022/Problem Sets/HomeWork1")
install.packages("readstata13")
library(readstata13)
######     Question 1    ######
## a.)
install.packages("readstata13")
library(readstata13)
fakenews <- read.dta13("fakenews.dta")
survey <- read.dta13("survey.dta")
######     Question 2    ######
help("na.omit")
# x = number of times pro-Trump articles were shared on Facebook
fakenews <- na.omit(fakenews)
x <- sum(fakenews$fb_share[fakenews$pro == "Trump"])
x
# y = number of times pro-Clinton articles were shared on Facebook
y <- sum(fakenews$fb_share[fakenews$pro == "Clinton"])
y
# Using cat() function to make a dynamic print message
# Multiplying x and y by 1000 since number of articles was in 1000s
cat(x*1000,"pro-Trump articles were shared on Facebook, and ", y*1000, "pro-Clinton articles were shared, in this dataset.")
######   Question 3   ######
# creating value 'Totalbuzz' for number of articles in fakenews
Totalbuzz <- sum(fakenews$buzzfeed == 1)
# creating value 'Justbuzz' for buzz articles that are not in any other data base
Justbuzz <- Totalbuzz - sum(fakenews$buzzfeed == 1 & (fakenews$politifact == 1 | fakenews$snopes == 1))
# percent of buzz articles that dont appear in others
Justbuzz / Totalbuzz
Totalsnopes <- sum(fakenews$snopes == 1)
Justsnopes <- Totalsnopes - sum(fakenews$snopes == 1 & (fakenews$politifact == 1 | fakenews$buzzfeed == 1))
Justsnopes / Totalsnopes
Totalpol <- sum(fakenews$politifact == 1)
Justpol <- Totalpol - sum(fakenews$politifact == 1 & (fakenews$snopes ==1 | fakenews$buzzfeed ==1))
Justpol / Totalpol
######   Question 4   ######
# Package to make numbers percentages
install.packages("formattable")
library(formattable)
TotalMinutes <- sum(survey$MediaMinutesPerDay)
TotalMinutes
SocialMinutes <- sum(survey$SocialMediaMinutesPerDay)
SocialMinutes
#using percent function with 0 allowable decimals
SocialPercent <- percent(SocialMinutes/TotalMinutes,0)
cat("Respondents reported spending a total of ", TotalMinutes, " minutes consuming
election news, and ",SocialPercent, "of that time was spent on Social Media" )
install.packages("readstata13")
AvgMinutes <- mean(survey$MediaMinutesPerDay)
AvgMinutes
View(survey)
AvgSocialMinutes <- sum(survey$SocialMediaMinutesPerDay)
AvgSocialMinutes
AvgSocialMinutes <- mean(survey$SocialMediaMinutesPerDay)
AvgSocialMinutes
SocialPercent <- percent(AvgSocialMinutes/AvgMinutes,0)
rep(1, nrow(survey))
a
install.packages("formattable")
library(formattable)
AvgMinutes <- round(mean(survey$MediaMinutesPerDay),digits=0)
AvgMinutes
#average social media minute when media minutes was greater than 0
AvgSocialMinutes <- mean(survey$SocialMediaMinutesPerDay[survey$MediaMinutesPerDay > 0])
a <- round((AvgSocialMinutes / AvgMinutes)*100,digits=0)
a
AvgMinutes
rm(list = ls())
setwd("/Users/elis_imac/Desktop/ECON-124_Winter_2022/Problem Sets/HomeWork1")
install.packages("readstata13")
library(readstata13)
fakenews <- read.dta13("fakenews.dta")
survey <- read.dta13("survey.dta")
View(fakenews)
x <- sum(na.omit(fakenews$fb_share[fakenews$pro == "Trump"]))
x
# y = number of times pro-Clinton articles were shared on Facebook
y <- sum(na.omit(fakenews$fb_share[fakenews$pro == "Clinton"]))
y
cat(x*1000,"pro-Trump articles were shared on Facebook, and ", y*1000, "pro-Clinton articles were shared, in this dataset.")
Totalbuzz <- sum(fakenews$buzzfeed == 1)
# creating value 'Justbuzz' for buzz articles that are not in any other data base
Justbuzz <- Totalbuzz - sum(fakenews$buzzfeed == 1 & (fakenews$politifact == 1 | fakenews$snopes == 1))
# percent of buzz articles that dont appear in others
Justbuzz / Totalbuzz
#Same Logic for Buzz operations above
Totalsnopes <- sum(fakenews$snopes == 1)
Justsnopes <- Totalsnopes - sum(fakenews$snopes == 1 & (fakenews$politifact == 1 | fakenews$buzzfeed == 1))
Justsnopes / Totalsnopes
#Same Logic for Buzz operations above
Totalpol <- sum(fakenews$politifact == 1)
Justpol <- Totalpol - sum(fakenews$politifact == 1 & (fakenews$snopes ==1 | fakenews$buzzfeed ==1))
Justpol / Totalpol
Totalbuzz <- sum(fakenews$buzzfeed == 1)
# creating value 'Justbuzz' for buzz articles that are not in any other data base
Justbuzz <- Totalbuzz - sum(fakenews$buzzfeed == 1 & (fakenews$politifact == 1 | fakenews$snopes == 1))
# percent of buzz articles that dont appear in others
(Justbuzz / Totalbuzz)*100
#Same Logic for Buzz operations above
Totalsnopes <- sum(fakenews$snopes == 1)
Justsnopes <- Totalsnopes - sum(fakenews$snopes == 1 & (fakenews$politifact == 1 | fakenews$buzzfeed == 1))
(Justsnopes / Totalsnopes)*100
#Same Logic for Buzz operations above
Totalpol <- sum(fakenews$politifact == 1)
Justpol <- Totalpol - sum(fakenews$politifact == 1 & (fakenews$snopes ==1 | fakenews$buzzfeed ==1))
(Justpol / Totalpol)*100
install.packages("formattable")
library(formattable)
AvgMinutes <- round(mean(survey$MediaMinutesPerDay),digits=0)
AvgMinutes
#average social media minute when media minutes was greater than 0
AvgSocialMinutes <- mean(survey$SocialMediaMinutesPerDay[survey$MediaMinutesPerDay > 0])
AvgSocialMinutes
a <- round((AvgSocialMinutes / AvgMinutes)*100,digits=0)
a
#using percent function with 0 allowable decimals
#SocialPercent <- percent(AvgSocialMinutes/AvgMinutes,0)
cat("Respondents reported spending on average", AvgMinutes, " minutes consuming
election news, and ",a,"% of that time was spent on Social Media" )
AvgMinutes <- round(mean(survey$MediaMinutesPerDay),digits=0)
AvgMinutes
#average social media minute when media minutes was greater than 0
AvgSocialMinutes <- mean(survey$SocialMediaMinutesPerDay[survey$MediaMinutesPerDay > 0])
AvgSocialMinutes
a <- round((AvgSocialMinutes / AvgMinutes)*100,digits=0)
a
#using percent function with 0 allowable decimals
#SocialPercent <- percent(AvgSocialMinutes/AvgMinutes,0)
cat("Respondents reported spending on average", AvgMinutes, " minutes consuming
election news, and ",a,"% of that time was spent on Social Media" )
z <- tapply(rep(1, nrow(survey)), survey$MostImportantSource, sum)
# making a barplot with sort() function on z to sort observations least to most
# using "las=2" to get the column names under each respective bar
barplot(sort(z),las=2)
